{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Fxj1ye0Y1ZwC","sWeKuQTt1k_M","2hMUb2yK1pEB","ZXuPgGbGa2Nd","670dtom5sl10"],"toc_visible":true,"machine_shape":"hm","gpuType":"A100","mount_file_id":"1nYy2OQQ4os8qAJRaYQ2kwsbNjzw3zFyx","authorship_tag":"ABX9TyM7QF060g8m/oZks22EC8jM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Fix the Attention Mask?\n","# Train 4 Head 3 Layer Model\n","# Combining Attention values across heads - Avg\n","# Combining Attention values across layers - Matrix Multiply\n","\n","# Claim 1: when combining attention heads in the same layer, we can just take average attention\n","# Claim 2: when combining attention layers, we can take the matrix multiply of the attentions"],"metadata":{"id":"tVe55J-aw7aC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"nRR5bUiLRZ5a"}},{"cell_type":"code","source":["!pip install dgl torch_geometric torch\n","\n","# Install required python libraries\n","import os\n","\n","# Install PyTorch Geometric and other libraries\n","if 'IS_GRADESCOPE_ENV' not in os.environ:\n","    print(\"Installing PyTorch Geometric\")\n","    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","    !pip install -q torch-geometric\n","    print(\"Installing other libraries\")\n","    !pip install networkx\n","    !pip install lovely-tensors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLx8ARPERaaB","executionInfo":{"status":"ok","timestamp":1714340656785,"user_tz":-60,"elapsed":95820,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"39a34a44-a10f-4423-81a8-ca4638185152"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dgl\n","  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12, dgl\n","Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch_geometric-2.5.3\n","Installing PyTorch Geometric\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling other libraries\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n","Collecting lovely-tensors\n","  Downloading lovely_tensors-0.1.15-py3-none-any.whl (17 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lovely-tensors) (2.2.1+cu121)\n","Collecting lovely-numpy>=0.2.9 (from lovely-tensors)\n","  Downloading lovely_numpy-0.2.11-py3-none-any.whl (24 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.25.2)\n","Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.5.29)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (7.34.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (3.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (24.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->lovely-numpy>=0.2.9->lovely-tensors)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lovely-tensors) (2.1.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (2.8.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lovely-tensors) (1.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.16.0)\n","Installing collected packages: jedi, lovely-numpy, lovely-tensors\n","Successfully installed jedi-0.19.1 lovely-numpy-0.2.11 lovely-tensors-0.1.15\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import time\n","import math\n","import random\n","import itertools\n","from datetime import datetime\n","from typing import Mapping, Tuple, Sequence, List\n","\n","import pandas as pd\n","import networkx as nx\n","import numpy as np\n","import scipy as sp\n","\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n","from torch.optim import Adam\n","\n","import torch_geometric\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.datasets import Planetoid\n","\n","import torch_geometric.transforms as T\n","from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n","\n","from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n","\n","# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n","\n","import lovely_tensors as lt\n","lt.monkey_patch()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# import warnings\n","# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","# warnings.filterwarnings(\"ignore\", category=UserWarning)\n","# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","print(\"All imports succeeded.\")\n","print(\"Python version {}\".format(sys.version))\n","print(\"PyTorch version {}\".format(torch.__version__))\n","print(\"PyG version {}\".format(torch_geometric.__version__))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMiwOILkRgEP","outputId":"1e83443e-9aaf-4622-b4c0-33f3f2f246dd","executionInfo":{"status":"ok","timestamp":1714340664004,"user_tz":-60,"elapsed":7222,"user":{"displayName":"Batu El","userId":"11666366648103508022"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"]},{"output_type":"stream","name":"stdout","text":["All imports succeeded.\n","Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","PyTorch version 2.2.1+cu121\n","PyG version 2.5.3\n"]}]},{"cell_type":"code","source":["# Set random seed for deterministic results\n","\n","def seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed(0)\n","print(\"All seeds set.\")"],"metadata":{"id":"zuVTx4otRi6i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714340664004,"user_tz":-60,"elapsed":3,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"3413a2f1-30a2-413b-a451-9ad18d8e2eee"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["All seeds set.\n"]}]},{"cell_type":"markdown","source":["# Datasets"],"metadata":{"id":"g6R5GR7hRdEI"}},{"cell_type":"code","source":["from torch_geometric.datasets import WebKB, WikipediaNetwork\n","\n","DATASETS = {}\n","\n","# Chamelion & Squirrel\n","# Cora & Citeseer\n","# Cornell & Texas & Wisconsin\n","\n","## Mid Size Datasets\n","# Citation Networks\n","dataset = 'Cora'\n","dataset = Planetoid('/tmp/Cora', dataset)\n","data = dataset[0]\n","DATASETS['Cora'] = data\n","dataset = 'Citeseer'\n","dataset = Planetoid('/tmp/Citeseer', dataset)\n","data = dataset[0]\n","DATASETS['Citeseer'] = data\n","# Wikipedia Pages\n","dataset = 'Chameleon'\n","dataset = WikipediaNetwork(root='/tmp/Chameleon', name='Chameleon')\n","data = dataset[0]\n","DATASETS['Chameleon'] = data\n","dataset = 'Squirrel'\n","dataset = WikipediaNetwork(root='/tmp/Squirrel', name='Squirrel')\n","data = dataset[0]\n","DATASETS['Squirrel'] = data\n","### Small Sized Datasets\n","# Web Pages\n","dataset = WebKB(root='/tmp/Cornell', name='Cornell')\n","data = dataset[0]\n","DATASETS['Cornell'] = data\n","dataset = WebKB(root='/tmp/Texas', name='Texas')\n","data = dataset[0]\n","DATASETS['Texas'] = data\n","dataset = WebKB(root='/tmp/Wisconsin', name='Wisconsin')\n","data = dataset[0]\n","DATASETS['Wisconsin'] = data"],"metadata":{"id":"8PVfelu6ReJe","executionInfo":{"status":"ok","timestamp":1714340708349,"user_tz":-60,"elapsed":44347,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7970243-ac97-43f7-ff9b-4fea824e91fa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n","Processing...\n","Done!\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n"]}]},{"cell_type":"code","source":["# import tqdm\n","# ### Shortest Paths ###\n","# def get_shortest_path_matrix(adjacency_matrix):\n","#     graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n","#     shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n","#     shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n","#     return shortest_path_matrix\n","\n","# SHORTEST_PATHS = {}\n","# for data_key in tqdm.tqdm(DATASETS):\n","#   print(data_key)\n","#   data = DATASETS[data_key]\n","#   dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n","#   dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)\n","#   SHORTEST_PATHS[data_key] = dense_shortest_path_matrix\n","\n","# ### Save the Shortest Paths\n","# import pickle\n","# with open('sp_dict.pkl', 'wb') as f:\n","#     pickle.dump(SHORTEST_PATHS, f)\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/L65/shortest_paths/sp_dict.pkl', 'rb') as f:\n","    SHORTEST_PATHS = pickle.load(f)"],"metadata":{"id":"PqcrQ7FuVa8c","executionInfo":{"status":"ok","timestamp":1714340746985,"user_tz":-60,"elapsed":13558,"user":{"displayName":"Batu El","userId":"11666366648103508022"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["for data_key in DATASETS:\n","  data = DATASETS[data_key]\n","  data.dense_sp_matrix = SHORTEST_PATHS[data_key]\n","  data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n","  data.dense_adj = data.dense_adj.cuda() + torch.eye(data.dense_adj.shape[0]).cuda()\n","  data.dense_adj[data.dense_adj == 2] = 1\n","  data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n","  DATASETS[data_key] = data"],"metadata":{"id":"hzEswGTMXoOI","executionInfo":{"status":"ok","timestamp":1714344125286,"user_tz":-60,"elapsed":3722,"user":{"displayName":"Batu El","userId":"11666366648103508022"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["### Masks ###\n","\n","def generate_masks(num_nodes=None,num_runs=None,train_ratio=None, val_ratio=None):\n","    masks = { 'train_mask': np.zeros((num_nodes, num_runs), dtype=int),\n","              'val_mask': np.zeros((num_nodes, num_runs), dtype=int),\n","              'test_mask': np.zeros((num_nodes, num_runs), dtype=int)}\n","\n","    for run in range(num_runs):\n","        indices = np.arange(num_nodes)\n","        np.random.shuffle(indices)\n","        train_end = int(train_ratio * num_nodes)\n","        val_end = train_end + int(val_ratio * num_nodes)\n","        masks['train_mask'][indices[:train_end], run] = 1\n","        masks['val_mask'][indices[train_end:val_end], run] = 1\n","        masks['test_mask'][indices[val_end:], run] = 1\n","\n","    tensor_masks = {'train_mask': torch.tensor(masks['train_mask']),\n","                    'val_mask':torch.tensor(masks['val_mask']),\n","                    'test_mask':torch.tensor(masks['test_mask'])}\n","    return tensor_masks\n","\n","for data_key in DATASETS:\n","    data = DATASETS[data_key]\n","\n","    masks = generate_masks(num_nodes=data.x.shape[0], num_runs=10, train_ratio=0.4, val_ratio=0.3)\n","    data.train_mask = masks['train_mask'].bool()\n","    data.val_mask = masks['val_mask'].bool()\n","    data.test_mask = masks['test_mask'].bool()\n","\n","    if len(data.train_mask.shape)==1:\n","      print('Add 10 Masks')\n","    else:\n","      print('We have 10 Masks')\n","      print('Train Ratio:',(data.train_mask[:,0].sum() / len(data.train_mask[:,0])).item())\n","      print('Val Ratio:',(data.val_mask[:,0].sum() / len(data.val_mask[:,0])).item())\n","      print('Test Ratio:',(data.test_mask[:,0].sum() / len(data.test_mask[:,0])).item())"],"metadata":{"id":"vsqv_LTEU-2H","executionInfo":{"status":"aborted","timestamp":1714340709202,"user_tz":-60,"elapsed":3,"user":{"displayName":"Batu El","userId":"11666366648103508022"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Table 1: Dataset Statistics"],"metadata":{"id":"tPGL9tFORJCQ"}},{"cell_type":"code","source":["### Table 1 ###\n","### Dataset Statistics ###\n","import dgl\n","Homophily_Levels = {}\n","\n","for data_key in DATASETS:\n","  data = DATASETS[data_key]\n","  edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n","  g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n","  g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n","  Homophily_Levels[data_key] = {'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n","                                'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n","                                'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n","                                'Number of Nodes': int(g.num_nodes()),\n","                                'Number of Edges': int(g.num_edges())\n","                                }\n","df = pd.DataFrame(Homophily_Levels).round(1)\n","df"],"metadata":{"id":"O34_wDE-RLre","executionInfo":{"status":"aborted","timestamp":1714340709202,"user_tz":-60,"elapsed":3,"user":{"displayName":"Batu El","userId":"11666366648103508022"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"7JV4ZtidSENR"}},{"cell_type":"code","source":["# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n","\n","class GNNModel(Module):\n","\n","    def __init__(\n","            self,\n","            in_dim: int = data.x.shape[-1],\n","            hidden_dim: int = 128,\n","            num_heads: int = 1,\n","            num_layers: int = 1,\n","            out_dim: int = len(data.y.unique()),\n","            dropout: float = 0.5,\n","        ):\n","        super().__init__()\n","\n","        self.lin_in = Linear(in_dim, hidden_dim)\n","        self.lin_out = Linear(hidden_dim, out_dim)\n","        self.layers = ModuleList()\n","\n","        for layer in range(num_layers):\n","            self.layers.append(\n","                GCNConv(hidden_dim, hidden_dim)\n","            )\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index):\n","\n","        x = self.lin_in(x)\n","\n","        for layer in self.layers:\n","            # conv -> activation ->  dropout -> residual\n","            x_in = x\n","            x = layer(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, self.dropout, training=self.training)\n","            x = x_in + x\n","\n","        x = self.lin_out(x)\n","        return x.log_softmax(dim=-1)\n","\n","\n","class SparseGraphTransformerModel(Module):\n","    def __init__(\n","            self,\n","            in_dim: int = data.x.shape[-1],\n","            hidden_dim: int = 128,\n","            num_heads: int = 1,\n","            num_layers: int = 1,\n","            out_dim: int = len(data.y.unique()),\n","            dropout: float = 0.5,\n","        ):\n","        super().__init__()\n","\n","        self.lin_in = Linear(in_dim, hidden_dim)\n","        self.lin_out = Linear(hidden_dim, out_dim)\n","\n","        self.layers = ModuleList()\n","        for layer in range(num_layers):\n","            self.layers.append(\n","                MultiheadAttention(\n","                    embed_dim = hidden_dim,\n","                    num_heads = num_heads,\n","                    dropout = dropout\n","                )\n","            )\n","        self.dropout = dropout\n","\n","    def forward(self, x, dense_adj):\n","\n","        x = self.lin_in(x)\n","\n","        self.attn_weights_list = []\n","\n","        for layer in self.layers:\n","            x_in = x\n","            x, attn_weights = layer(\n","                x, x, x,\n","                attn_mask = ~dense_adj.bool(),\n","                average_attn_weights = False\n","            )\n","            x = F.relu(x)\n","            x = F.dropout(x, self.dropout, training=self.training)\n","            x = x_in + x\n","\n","            self.attn_weights_list.append(attn_weights)\n","\n","        x = self.lin_out(x)\n","\n","        return x.log_softmax(dim=-1)\n","\n","class DenseGraphTransformerModel(Module):\n","\n","    def __init__(\n","            self,\n","            in_dim: int = data.x.shape[-1],\n","            pos_enc_dim: int = 16,\n","            hidden_dim: int = 128,\n","            num_heads: int = 1,\n","            num_layers: int = 1,\n","            out_dim: int = len(data.y.unique()),\n","            dropout: float = 0.5,\n","        ):\n","        super().__init__()\n","\n","        self.lin_in = Linear(in_dim, hidden_dim)\n","        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n","        self.lin_out = Linear(hidden_dim, out_dim)\n","\n","        self.layers = ModuleList()\n","        for layer in range(num_layers):\n","            self.layers.append(\n","                MultiheadAttention(\n","                    embed_dim = hidden_dim,\n","                    num_heads = num_heads,\n","                    dropout = dropout\n","                )\n","            )\n","\n","\n","        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n","        self.dropout = dropout\n","\n","    def forward(self, x, pos_enc, dense_sp_matrix):\n","\n","        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n","        x = self.lin_in(x)  # no node positional encoding\n","\n","        # attention bias\n","        # [i, j] -> inverse of shortest path distance b/w node i and j\n","        # diagonals -> self connection, set to 0\n","        # disconnected nodes -> -1\n","        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n","            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n","            nan=0, posinf=0, neginf=0)\n","        #attn_bias = torch.ones_like(attn_bias)\n","\n","        # TransformerEncoder\n","        # x = self.encoder(x, mask = attn_bias)\n","\n","        self.attn_weights_list = []\n","\n","        for layer in self.layers:\n","            # MHSA layer\n","            # float mask adds learnable additive attention bias\n","            x_in = x\n","            x, attn_weights = layer(\n","                x, x, x,\n","                attn_mask = attn_bias,\n","                average_attn_weights = False\n","            )\n","            x = F.relu(x)\n","            x = F.dropout(x, self.dropout, training=self.training)\n","            x = x_in + x\n","\n","            self.attn_weights_list.append(attn_weights)\n","\n","        x = self.lin_out(x)\n","\n","        return x.log_softmax(dim=-1)\n","\n","\n","\n","class DenseGraphTransformerModel_V2(Module):\n","    def __init__(\n","            self,\n","            in_dim: int = data.x.shape[-1],\n","            pos_enc_dim: int = 16,\n","            hidden_dim: int = 128,\n","            num_heads: int = 1,\n","            num_layers: int = 1,\n","            out_dim: int = len(data.y.unique()),\n","            dropout: float = 0.5,\n","        ):\n","        super().__init__()\n","\n","        self.lin_in = Linear(in_dim, hidden_dim)\n","        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n","        self.lin_out = Linear(hidden_dim, out_dim)\n","\n","        self.layers = ModuleList()\n","        for layer in range(num_layers):\n","            self.layers.append(\n","                MultiheadAttention(\n","                    embed_dim = hidden_dim,\n","                    num_heads = num_heads,\n","                    dropout = dropout\n","                )\n","            )\n","\n","        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n","        self.dropout = dropout\n","\n","    def forward(self, x, pos_enc, dense_sp_matrix):\n","\n","        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n","        # x = self.lin_in(x)  # no node positional encoding\n","\n","        # attention bias\n","        # [i, j] -> inverse of shortest path distance b/w node i and j\n","        # diagonals -> self connection, set to 0\n","        # disconnected nodes -> -1\n","        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n","        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n","        #     nan=0, posinf=0, neginf=0\n","        # )\n","        #attn_bias = torch.ones_like(attn_bias)\n","\n","        # TransformerEncoder\n","        # x = self.encoder(x, mask = attn_bias)\n","\n","        self.attn_weights_list = []\n","\n","        for layer in self.layers:\n","            # # TransformerEncoderLayer\n","            # # float mask adds learnable additive attention bias\n","            # x = layer(x, src_mask = attn_bias)\n","\n","            # MHSA layer\n","            # float mask adds learnable additive attention bias\n","            x_in = x\n","            x, attn_weights = layer(\n","                x, x, x,\n","                # attn_mask = attn_bias,\n","                average_attn_weights = False\n","            )\n","            x = F.relu(x)\n","            x = F.dropout(x, self.dropout, training=self.training)\n","            x = x_in + x\n","\n","            self.attn_weights_list.append(attn_weights)\n","\n","        x = self.lin_out(x)\n","\n","        return x.log_softmax(dim=-1)"],"metadata":{"id":"AKL9b7tCSDDd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trainers"],"metadata":{"id":"Z39ODu9oT2f9"}},{"cell_type":"code","source":["def Train_GCN(NUM_LAYERS,\n","              NUM_HEADS,\n","              data):\n","\n","    IN_DIM = data.x.shape[-1]\n","    OUT_DIM = len(data.y.unique())\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model = GNNModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n","    data = data.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","    def train():\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data.x, data.edge_index)\n","        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","        return float(loss)\n","\n","    @torch.no_grad()\n","    def test():\n","        model.eval()\n","        pred, accs = model(data.x, data.edge_index).argmax(dim=-1), []\n","        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","        return accs\n","\n","    best_val_acc = test_acc = 0\n","    times = []\n","    for epoch in range(1, 100):\n","        start = time.time()\n","        loss = train()\n","        train_acc, val_acc, tmp_test_acc = test()\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            test_acc = tmp_test_acc\n","        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n","        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n","        #       f'Final Test: {test_acc:.4f}')\n","        times.append(time.time() - start)\n","    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n","    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, None\n","\n","def Train_SparseGraphTransformerModel(NUM_LAYERS,\n","              NUM_HEADS,\n","              data):\n","\n","    IN_DIM = data.x.shape[-1]\n","    OUT_DIM = len(data.y.unique())\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model = SparseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n","    data = data.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    def train():\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data.x, data.dense_adj)\n","        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","        return float(loss)\n","\n","    @torch.no_grad()\n","    def test():\n","        model.eval()\n","        pred, accs = model(data.x, data.dense_adj).argmax(dim=-1), []\n","        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","        return accs\n","\n","    best_val_acc = test_acc = 0\n","    times = []\n","    for epoch in range(1, 100):\n","        start = time.time()\n","        loss = train()\n","        train_acc, val_acc, tmp_test_acc = test()\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            test_acc = tmp_test_acc\n","        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n","        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n","        #       f'Final Test: {test_acc:.4f}')\n","        times.append(time.time() - start)\n","    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n","    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n","\n","def Train_DenseGraphTransformerModel(NUM_LAYERS,\n","              NUM_HEADS,\n","              data):\n","\n","    IN_DIM = data.x.shape[-1]\n","    OUT_DIM = len(data.y.unique())\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model = DenseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n","    data = data.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    def train():\n","        model.train()\n","        optimizer.zero_grad()\n","        # print(data.pos_enc)\n","        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n","        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","        return float(loss)\n","\n","\n","    @torch.no_grad()\n","    def test():\n","        model.eval()\n","        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n","        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","        return accs\n","\n","    best_val_acc = test_acc = 0\n","    times = []\n","    for epoch in range(1, 100):\n","        start = time.time()\n","        loss = train()\n","        train_acc, val_acc, tmp_test_acc = test()\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            test_acc = tmp_test_acc\n","        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n","        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n","        #       f'Final Test: {test_acc:.4f}')\n","        times.append(time.time() - start)\n","    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n","    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n","\n","    # Notes\n","    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n","    # - Node positional encodings are not particularly useful\n","    # - Edge distance encodings are very useful\n","    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes\n","\n","def Train_DenseGraphTransformerModel_V2(NUM_LAYERS,\n","              NUM_HEADS,\n","              data):\n","\n","    IN_DIM = data.x.shape[-1]\n","    OUT_DIM = len(data.y.unique())\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    model = DenseGraphTransformerModel_V2(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n","    data = data.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    def train():\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n","        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","        return float(loss)\n","\n","    @torch.no_grad()\n","    def test():\n","        model.eval()\n","        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n","        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n","        return accs\n","\n","    best_val_acc = test_acc = 0\n","    times = []\n","    for epoch in range(1, 100):\n","        start = time.time()\n","        loss = train()\n","        train_acc, val_acc, tmp_test_acc = test()\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            test_acc = tmp_test_acc\n","        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n","        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n","        #       f'Final Test: {test_acc:.4f}')\n","        times.append(time.time() - start)\n","    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n","    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n","\n","    # Notes\n","    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n","    # - Node positional encodings are not particularly useful\n","    # - Edge distance encodings are very useful\n","    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"],"metadata":{"id":"bgXWrDZ5V_al"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"veDrDxJIaO7f"}},{"cell_type":"markdown","source":["## Training: 1 Layer, 1 Head"],"metadata":{"id":"X3wGJPl3YJcd"}},{"cell_type":"code","source":["import tqdm\n","NUM_LAYERS = 1\n","NUM_HEADS = 1\n","NUM_RUNS = 10\n","all_stats = {}\n","for data_key in DATASETS:\n","    print(f'Training on {data_key}')\n","    data = DATASETS[data_key]\n","\n","    TRAIN_MASKS = data.train_mask\n","    VAL_MASKS = data.val_mask\n","    TEST_MASKS = data.test_mask\n","\n","    run_stats = {}\n","\n","    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n","        data.train_mask = TRAIN_MASKS[:,mask_idx]\n","        data.val_mask = VAL_MASKS[:,mask_idx]\n","        data.test_mask = TEST_MASKS[:,mask_idx]\n","\n","        accuracy_statistics = {}\n","        attn_weights = {}\n","\n","        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n","        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n","        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n","    all_stats[data_key] = run_stats\n","    data.train_mask = TRAIN_MASKS\n","    data.val_mask = VAL_MASKS\n","    data.test_mask = TEST_MASKS\n","\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n","    pickle.dump(all_stats, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6GoLurKW57q","executionInfo":{"status":"ok","timestamp":1710738973707,"user_tz":0,"elapsed":229201,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"5a2e6a7c-8104-4d38-cdff-fb422102fd9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on Cora\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Citeseer\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:22<00:00,  2.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Chameleon\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Squirrel\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:36<00:00,  3.60s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Cornell\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Texas\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:17<00:00,  1.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Wisconsin\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"]}]},{"cell_type":"markdown","source":["## Training: 1 Layer, 2 Heads"],"metadata":{"id":"Fxj1ye0Y1ZwC"}},{"cell_type":"code","source":["import tqdm\n","NUM_LAYERS = 1\n","NUM_HEADS = 2\n","NUM_RUNS = 10\n","all_stats = {}\n","for data_key in DATASETS:\n","    print(f'Training on {data_key}')\n","    data = DATASETS[data_key]\n","\n","    TRAIN_MASKS = data.train_mask\n","    VAL_MASKS = data.val_mask\n","    TEST_MASKS = data.test_mask\n","\n","    run_stats = {}\n","\n","    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n","        data.train_mask = TRAIN_MASKS[:,mask_idx]\n","        data.val_mask = VAL_MASKS[:,mask_idx]\n","        data.test_mask = TEST_MASKS[:,mask_idx]\n","\n","        accuracy_statistics = {}\n","        attn_weights = {}\n","\n","        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n","        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n","        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n","    all_stats[data_key] = run_stats\n","    data.train_mask = TRAIN_MASKS\n","    data.val_mask = VAL_MASKS\n","    data.test_mask = TEST_MASKS\n","\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n","    pickle.dump(all_stats, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1igyz0Pr1dRN","executionInfo":{"status":"ok","timestamp":1710739323962,"user_tz":0,"elapsed":350257,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"007dcfb4-f029-465a-81fe-05b0ac2fb881"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on Cora\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:27<00:00,  2.73s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Citeseer\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Chameleon\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:24<00:00,  2.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Squirrel\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:52<00:00,  5.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Cornell\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Texas\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Wisconsin\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:18<00:00,  1.82s/it]\n"]}]},{"cell_type":"markdown","source":["## Training: 2 Layers, 1 Head"],"metadata":{"id":"sWeKuQTt1k_M"}},{"cell_type":"code","source":["import tqdm\n","### Train Cora ###\n","NUM_LAYERS = 2\n","NUM_HEADS = 1\n","NUM_RUNS = 10\n","# data_key = 'Wisconsin'\n","all_stats = {}\n","for data_key in DATASETS:\n","    print(f'Training on {data_key}')\n","    data = DATASETS[data_key]\n","\n","    TRAIN_MASKS = data.train_mask\n","    VAL_MASKS = data.val_mask\n","    TEST_MASKS = data.test_mask\n","\n","    run_stats = {}\n","\n","    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n","        data.train_mask = TRAIN_MASKS[:,mask_idx]\n","        data.val_mask = VAL_MASKS[:,mask_idx]\n","        data.test_mask = TEST_MASKS[:,mask_idx]\n","\n","        accuracy_statistics = {}\n","        attn_weights = {}\n","\n","        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n","        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel'])\n","        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel'])\n","        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2'])\n","        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n","    all_stats[data_key] = run_stats\n","    data.train_mask = TRAIN_MASKS\n","    data.val_mask = VAL_MASKS\n","    data.test_mask = TEST_MASKS\n","\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n","    pickle.dump(all_stats, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcAfw7My1oij","executionInfo":{"status":"ok","timestamp":1710739717256,"user_tz":0,"elapsed":393297,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"9a14a337-b5e8-458a-8573-4ce8a7a50711"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on Cora\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:29<00:00,  2.91s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Citeseer\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:33<00:00,  3.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Chameleon\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Squirrel\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:56<00:00,  5.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Cornell\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Texas\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Wisconsin\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:27<00:00,  2.74s/it]\n"]}]},{"cell_type":"markdown","source":["## Training: 2 Layers, 2 Heads"],"metadata":{"id":"2hMUb2yK1pEB"}},{"cell_type":"code","source":["import gc\n","import tqdm\n","### Train Cora ###\n","NUM_LAYERS = 2\n","NUM_HEADS = 2\n","NUM_RUNS = 10\n","# data_key = 'Wisconsin'\n","all_stats = {}\n","for data_key in DATASETS:\n","    print(f'Training on {data_key}')\n","    data = DATASETS[data_key]\n","\n","    TRAIN_MASKS = data.train_mask\n","    VAL_MASKS = data.val_mask\n","    TEST_MASKS = data.test_mask\n","\n","    run_stats = {}\n","\n","    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n","        gc.collect()\n","        data.train_mask = TRAIN_MASKS[:,mask_idx]\n","        data.val_mask = VAL_MASKS[:,mask_idx]\n","        data.test_mask = TEST_MASKS[:,mask_idx]\n","\n","        accuracy_statistics = {}\n","        attn_weights = {}\n","\n","        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n","        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n","        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n","        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n","        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n","\n","    all_stats[data_key] = run_stats\n","    data.train_mask = TRAIN_MASKS\n","    data.val_mask = VAL_MASKS\n","    data.test_mask = TEST_MASKS\n","\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n","    pickle.dump(all_stats, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYmZVaET1sKD","executionInfo":{"status":"ok","timestamp":1710740360315,"user_tz":0,"elapsed":643061,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"e7aec9f4-9e13-4f97-f939-e114ca8fc868"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on Cora\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:48<00:00,  4.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Citeseer\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:58<00:00,  5.83s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Chameleon\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:42<00:00,  4.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Squirrel\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [01:35<00:00,  9.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Cornell\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Texas\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:29<00:00,  2.99s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training on Wisconsin\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n"]}]},{"cell_type":"markdown","source":["# Analysis\n"],"metadata":{"id":"ZXuPgGbGa2Nd"}},{"cell_type":"markdown","source":["## Table 2: Accuracy Statistics"],"metadata":{"id":"670dtom5sl10"}},{"cell_type":"code","source":["### Table 2 ###\n","### Accuracy Statistics ###\n","pd.set_option('display.max_columns', None)\n","\n","all_stats_df = {}\n","for data_key in all_stats:\n","  run_stats = all_stats[data_key]\n","  table1 = pd.concat({key : pd.DataFrame(run_stats[key]['accuracy']) for key in run_stats}, axis=0)\n","  table1_train = pd.concat({'mean': table1.mean(level=1, axis=0).loc['train_acc'], 'std':table1.std(level=1).loc['train_acc']}, axis=1)\n","  table1_test = pd.concat({'mean': table1.mean(level=1, axis=0).loc['test_acc'], 'std':table1.std(level=1).loc['test_acc']}, axis=1)\n","  # table1 = pd.concat({'Train': table1_train, 'Test': table1_test}, axis=1)\n","  table1 = table1_test\n","  all_stats_df[data_key] = table1\n","pd.concat(all_stats_df, axis=1).round(2)"],"metadata":{"id":"ZNkbWu7zH06U"},"execution_count":null,"outputs":[]}]}